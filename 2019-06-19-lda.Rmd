---
title: "Untitled"
author: "Ashwin Malshe"
date: "6/19/2019"
output: tint::tintHtml
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Topic Modeling

Topic modeling allows us to identify topics embedded in the textual data. The assumption is that each text document is composed of one or more topics, which are latent. The job of a topic model then is to extract the topics from text. In this chapter we will use a popular probabilistic model called Latent Dirichlet Allocation (LDA) first introduced by Blei, Ng, and Jordan (2003).^[Blei, David M (2003), "Latent Dirichlet Allocation," Journal of Machine Learning Research, 3, 993–1022.] LDA is a unsupervised machine learning method as we don't know the target variable (i.e., the latent topic) ex ante. 

## Latent Dirichlet Allocation

Imagine that you are in your dentist's waiting room. You pick up a magazine and casually start browsing it. While eyeballing text on random pages, you came across the following text:

_Williamson and Taylor then began to reap the rewards of their patience and accelerated their way to an immensely productive partnership of 160 from 28.5 overs before Taylor chipped to Jason Holder at mid-on off the bowling of Chris Gayle to depart for 69. Williamson remained steady as ever and went on to record his second consecutive hundred, following on from his knock against South Africa on Wednesday. The Kiwi skipper eventually fell for 148, amassing over half of his side’s final total of 291. Cottrell was West Indies’ leading man, finishing with figures of 4/56._

_With Evin Lewis suffering an injury to his hamstring, Shai Hope joined Chris Gayle at the crease to begin the chase, but the right-hander perished early to Trent Boult, and Nicholas Pooran followed him back to the sheds not long after. Gayle took a liking to Matt Henry’s bowling and his partnership with Shimron Hetmyer – featuring some monstrous sixes – saw West Indies take control of the match. The game then swung back in the Black Caps’ favour as Lockie Ferguson interrupted with the removal of Hetmyer with an incredible slower ball that initiated a collapse of five wickets for 22 runs._^[Source: https://www.cricketworldcup.com/news/en/1253879]

Unless you are from the UK, Indian Subcontinent, Australia, New Zealand, South Africa, or the Carribian, there is little chance you understood anything meaningful in this text! However, some of these words look familiar to you: **ball**, **total**, **match**, **game**, and **runs**. These words give you a hint about the topic underlying this text. This looks like a sports article discussing a game between South Africa and West Indies. Indeed, after some Google search, you find out that this is game of Cricket.

Note that in order to determine the topic underlying the text, you used some of the words in the text. In an article that describes a game of Cricket, it is likely that the article will use words associated with Cricket. However, some of these words also may appear in an article about Baseball. So there is some uncertainty in your mind about the topic of the text. You decide to assign 60-40 probabilities to Cricket and Baseball.

LDA works on a similar principle. It assumes a data generating process under which a document is generated based on a mix of latent topics and the words that pertain to those topics. As such LDA trats an article as a "bag of words". LDA ignores the ordering of those words. Thus, for LDA both these sentences are the same:

*Williamson remained steady as ever and went on to record his second consecutive hundred, following on from his knock against South Africa on Wednesday.*

and

*steady remained Williamson as ever and record on to went his hundred second consecutive Wednesday against Africa South on, knock following on from his. *

LDA assumes that each document has a set of topics, which follow multinomial logistic distribution. However, the probabilities of the multinomial model are not fixed for all the documents. LDA assumes that the distribution of probabilities follow Dirchlet distribution. Thus, for each document, the topics are random draws from a multinomial distribution. The probabilities of the multinomial distribution are in turn are random draws from Drichlet distribution. The choice of this distribution is due to mathematical convinience as Dirichlet distribution is a conjugate prior to miltinomial logistic distribution. As a result, the ***posterior*** distribution of the probability distribution is Dirichlet too. This significantly simplifies the inference problem.^[For a partial mathematical treatment please refer to the original paper cited above.]

Our task in topic modeling using LDA can be broken down in the following steps:

1. Create a corpus of multiple text documents.

2. Preprocess the text to remove numbers, stop words, punctuations, etc. Additionally, use stemming.

3. Decide the number of topics and fit LDA on the corpus. The number of topics is a hyperparameter to tune.

4. Get the most common words defining each topic. Give them meaningful labels.




## Amazon reviews



```{r}
pacman::p_load(dplyr, ggplot2, tm, topicmodels, qdap, caret, here)
```

```{r echo=FALSE, eval=FALSE}
reviews <- read.csv("1429_1.csv",
                    stringsAsFactors = FALSE) %>% 
  filter(qdap::word_count(.$reviewText, byrow = TRUE) >= 50)
```


```{r}
names(reviews)
```

```{r}
head(reviews)
```

We will use only the review text.

```{r}
review_text <- reviews %>% pull(reviewText)
```

Get stop words from this discussion on Github:


```{r}
my_stopwords <- readRDS("my-stopwords.rds")
```

Create a corpus

## Preprocessing

```{r eval=FALSE}
rev_corpus <- tm::Corpus(VectorSource(review_text)) %>% 
  tm_map(content_transformer(tolower)) %>% 
  tm_map(removeWords, c(my_stopwords, "amazon")) %>% 
  tm_map(removeNumbers) %>% 
  tm_map(stripWhitespace) %>% 
  tm_map(removePunctuation, preserve_intra_word_dashes = TRUE) %>% 
  tm_map(stemDocument)
  

```

Create a document term matrix. Bounds specifies dropping the terms that appear in documents fewer than the lower bound and more than the upper bound.

```{r eval=FALSE}
dtm <- rev_corpus %>% 
  DocumentTermMatrix(control = list(bounds = list(global = c(100, 1000)))) 
```


```{r}
dim(dtm)
```


```{r eval=FALSE}
index_kp <- rowSums(as.matrix(dtm)) > 0
```


```{r}
sum(index_kp)
```

```{r}
dtm <- dtm[index_kp, ]
review_text <- review_text[index_kp]
```

Takes about 1 minute or less

```{r eval=FALSE}

lda_model <- LDA(x = dtm,
                 k = 20,
                 method = "Gibbs",
                 control = list(seed = 5648,
                                alpha = 0.2,
                                iter = 2000,
                                burnin = 1000)
                 )
```



```{r}
terms(lda_model, 10)
```

```{r}
lda_post <- posterior(lda_model)
```

```{r}
names(lda_post)
```

```{r}
beta <- lda_post$terms
theta <- lda_post$topics
```

```{r}
colMeans(theta)
```


```{r}
theta_rating <- as.data.frame(theta)
names(theta_rating) <- paste0("topic", 1:20)

theta_rating$rating <- reviews$overall[index_kp]
```


```{r}
summary(lm(rating ~ . - topic1,
           data = theta_rating))
```

## Random Forest

```{r}
theta_rating$rating_ord <- factor(theta_rating$rating,
                                     ordered = TRUE)
```

```{r}
class(theta_rating$rating_ord)
```

```{r}
trControl <- trainControl(method = "cv",
                          number = 10,
                          p = 0.8)

tuneGrid <- expand.grid(mtry = 1:18)
```

Don't run this in the class

```{r eval=FALSE}

set.seed(9403)
model_rf <- train(rating_ord ~ . - topic1 - rating,
                  data = theta_rating,
                  method = "rf",
                  trControl = trControl,
                  tuneGrid = tuneGrid
                  )
```


```{r}
model_rf$bestTune
```


Create a confusion matrix

```{r eval=FALSE}
rf_predict <- predict(model_rf, theta_rating)
```


```{r}
confusionMatrix(rf_predict,
                reference = theta_rating$rating_ord)
```

```{r}
varImp(model_rf, scale = TRUE)
```
